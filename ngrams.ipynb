{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e323b6db-bbfb-49d5-a6e8-029748daedc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "from nltk import ngrams\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e45251c-f788-4955-a45a-ca325c861047",
   "metadata": {},
   "source": [
    "Bag of words implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2b0f9dfb-8843-401a-9be0-3c098ea4c834",
   "metadata": {},
   "outputs": [],
   "source": [
    "def createBagsOfWords(posSentences, negSentences):\n",
    "    \"posSentences, negSentences : array of tweets (tweet = array of words)\"\n",
    "    pos = {}\n",
    "    neg = {}\n",
    "    for sentence in posSentences:\n",
    "        for word in sentence:\n",
    "            pos[word] = pos.get(word, 0) + 1\n",
    "    for sentence in negSentences:\n",
    "        for word in sentence:\n",
    "            neg[word] = neg.get(word, 0) + 1\n",
    "    return (pos, neg)\n",
    "                \n",
    "def likelihoodFromBags(posBag, negBag):\n",
    "    posWords = set(posBag.keys()) #set for union later on\n",
    "    negWords = set(negBag.keys()) \n",
    "    allWords = posWords.union(negWords)\n",
    "    \n",
    "    posLikelihood = {}\n",
    "    negLikelihood = {}\n",
    "\n",
    "    \n",
    "    for word in allWords:\n",
    "        posCount = posBag.get(word, 0) + 1 # +1 for laplace smoothing, not necessary\n",
    "        negCount = negBag.get(word, 0) + 1\n",
    "        total = posCount + negCount\n",
    "        posLikelihood[word] = posCount / total\n",
    "        negLikelihood[word] = negCount / total\n",
    "        \n",
    "    return (posLikelihood, negLikelihood)\n",
    "\n",
    "def likelihoodFromSentences(posSentences, negSentences):\n",
    "    (posBag, negBag) = createBagsOfWords(posSentences, negSentences)\n",
    "    return likelihoodFromBags(posBag, negBag)\n",
    "\n",
    "def estimateForNGram(ngram, posLiks, negLiks):\n",
    "    posLikelihood = 1.0\n",
    "    negLikelihood = 1.0\n",
    "    for word in ngram:\n",
    "        posLikelihood *= posLiks.get(word, 1.0) #if absent in one, will be absent in other => ignore word by * 1\n",
    "        negLikelihood *= negLiks.get(word, 1.0)\n",
    "    if posLikelihood >= negLikelihood: #all words unknown or equality => positive by default, let's be optimistic\n",
    "        return 1\n",
    "    else:\n",
    "        return -1\n",
    "    \n",
    "def predictFromScratch(train_pos, train_neg, test):\n",
    "    (posLiks, negLiks) = likelihoodFromSentences(train_pos, train_neg)\n",
    "    return [estimateForNGram(x, posLiks, negLiks) for x in test]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bac176f5-dcd6-48c0-aed7-46d5426e0665",
   "metadata": {},
   "source": [
    "File interaction and data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "acff477a-f0ce-4270-8b88-90c460597ccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_grams_length = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e15c66dc-1ddf-457f-95a3-53d2177a3f50",
   "metadata": {},
   "outputs": [],
   "source": [
    "def string2ngrams(text, n):\n",
    "    n_grams = ngrams(text.split(), n)\n",
    "    return [' '.join(grams) for grams in n_grams]\n",
    "\n",
    "def readFileOfTweets(path, isTestData=False):\n",
    "    ret = None\n",
    "    with open(path, \"r\") as f:\n",
    "        ret = f.read().splitlines()\n",
    "    return ret\n",
    "\n",
    "def fileOfTweetsToNgramSentences(path, isTestData=False):\n",
    "    strings = readFileOfTweets(path, isTestData)\n",
    "    return [string2ngrams(x, n_grams_length) for x in strings]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "12a7f53c-941c-4e0e-a4a3-0920d2b77055",
   "metadata": {},
   "outputs": [],
   "source": [
    "def crossValidate(posNgrams, negNgrams, fold):\n",
    "    cv_len = len(posNgrams) // fold\n",
    "    accuracy = 0.0\n",
    "    for i in range(fold):\n",
    "        posNgramTest, posNgramTrain = posNgrams[i*cv_len:(i+1)*cv_len], (posNgrams[:i*cv_len] + posNgrams[(i+1)*cv_len:])\n",
    "        negNgramTest, negNgramTrain = negNgrams[i*cv_len:(i+1)*cv_len], (negNgrams[:i*cv_len] + negNgrams[(i+1)*cv_len:])\n",
    "        \n",
    "        (posLiks, negLiks) = likelihoodFromSentences(posNgramTrain, negNgramTrain)\n",
    "\n",
    "        predictPositive = [estimateForNGram(x, posLiks, negLiks) for x in posNgramTest]\n",
    "        tp = len([x for x in predictPositive if x == 1])\n",
    "        predictNegative = [estimateForNGram(x, posLiks, negLiks) for x in negNgramTest]\n",
    "        tn = len([x for x in predictNegative if x == -1])\n",
    "        \n",
    "        print(tp, tn)\n",
    "        \n",
    "        accuracy += ((tp + tn) / (2*cv_len))\n",
    "        \n",
    "    print(\"accuracy : \", accuracy / fold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5936abd7-cce5-4865-bd97-8c72c647f9b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "221863 191482\n",
      "222696 191073\n",
      "221098 190549\n",
      "222130 191173\n",
      "221092 191481\n",
      "accuracy :  0.8258547999999999\n"
     ]
    }
   ],
   "source": [
    "posNgrams = fileOfTweetsToNgramSentences(\"./data/train_pos_full.txt\")\n",
    "negNgrams = fileOfTweetsToNgramSentences(\"./data/train_neg_full.txt\")\n",
    "\n",
    "crossValidate(posNgrams, negNgrams, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b1ac3d06-726b-4e7f-92bf-f8d91dcccd78",
   "metadata": {},
   "outputs": [],
   "source": [
    "def readTestFile():\n",
    "    with open(\"data/test_data.txt\", \"r\") as f:\n",
    "        content = f.readlines()\n",
    "    content = [re.split(\",\", x, maxsplit=1)[1] for x in content]\n",
    "    return content\n",
    "\n",
    "def publishResults(test_pred, file_name):\n",
    "    with open('data/' + file_name, \"w\") as f:\n",
    "        f.write(\"Id,Prediction\\n\")\n",
    "        for pred, index in zip(test_pred, range(1, len(test_pred) +1)):\n",
    "            f.write(str(index) + \",\" + str(pred) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b3cb234c-00d3-4d76-95f6-496274b5d643",
   "metadata": {},
   "outputs": [],
   "source": [
    "(posLiks, negLiks) = likelihoodFromSentences(posNgrams, negNgrams)\n",
    "test_x = [string2ngrams(x, n_grams_length) for x in readTestFile()]\n",
    "publishResults([estimateForNGram(x, posLiks, negLiks) for x in test_x], str(n_grams_length) + \"-grams.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44be4c30-07b6-46a9-bff7-1206eedd555e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
